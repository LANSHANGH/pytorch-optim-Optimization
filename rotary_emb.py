import torch
import triton
import triton.language as tl
from triton.testing import do_bench
import pandas as pd

# ==============================================================================
#  å†…æ ¸å®šä¹‰
# ==============================================================================

# å†…æ ¸1: æ…¢é€Ÿ/é€šç”¨è·¯å¾„ (Slow/Generic Path) - å¤„ç†éè¿ç»­å¼ é‡
@triton.jit
def _rope_kernel_rowwise(x_ptr, freqs_cis_ptr, output_ptr,
                         stride_x_b, stride_x_h, stride_x_m, stride_x_n,
                         stride_freq_m, stride_freq_n_pair, stride_freq_complex,
                         stride_out_b, stride_out_h, stride_out_m, stride_out_n,
                         N_dim, BLOCK_SIZE_N: tl.constexpr, OUTPUT_DTYPE: tl.constexpr):
    # æ¯ä¸ªç¨‹åºå¤„ç† (B, H, M) ç»´åº¦ä¸Šçš„ä¸€ä¸ª "è¡Œ"
    pid_b, pid_h, pid_m = tl.program_id(0), tl.program_id(1), tl.program_id(2)

    # åŸºäºå¤šç»´ stride è®¡ç®—è¡ŒæŒ‡é’ˆ
    row_x_ptr = x_ptr + pid_b * stride_x_b + pid_h * stride_x_h + pid_m * stride_x_m
    row_out_ptr = output_ptr + pid_b * stride_out_b + pid_h * stride_out_h + pid_m * stride_out_m
    row_freq_ptr = freqs_cis_ptr + pid_h * stride_freq_m

    # åœ¨ N ç»´åº¦ä¸Šåˆ†å—å¤„ç†
    offs_n_pairs = tl.arange(0, BLOCK_SIZE_N // 2)
    mask_n_pairs = offs_n_pairs < N_dim // 2

    x_real_ptrs = row_x_ptr + (2 * offs_n_pairs) * stride_x_n
    x_imag_ptrs = row_x_ptr + (2 * offs_n_pairs + 1) * stride_x_n
    x_real_half = tl.load(x_real_ptrs, mask=mask_n_pairs, other=0.0)
    x_imag_half = tl.load(x_imag_ptrs, mask=mask_n_pairs, other=0.0)

    freq_real_ptrs = row_freq_ptr + offs_n_pairs * stride_freq_n_pair
    freq_imag_ptrs = row_freq_ptr + offs_n_pairs * stride_freq_n_pair + stride_freq_complex
    freq_real = tl.load(freq_real_ptrs, mask=mask_n_pairs, other=0.0)
    freq_imag = tl.load(freq_imag_ptrs, mask=mask_n_pairs, other=0.0)

    x_real_fp32, x_imag_fp32 = x_real_half.to(tl.float32), x_imag_half.to(tl.float32)
    y_real_fp32 = x_real_fp32 * freq_real - x_imag_fp32 * freq_imag
    y_imag_fp32 = x_real_fp32 * freq_imag + x_imag_fp32 * freq_real

    y_real_out, y_imag_out = y_real_fp32.to(OUTPUT_DTYPE), y_imag_fp32.to(OUTPUT_DTYPE)
    out_real_ptrs = row_out_ptr + (2 * offs_n_pairs) * stride_out_n
    out_imag_ptrs = row_out_ptr + (2 * offs_n_pairs + 1) * stride_out_n
    tl.store(out_real_ptrs, y_real_out, mask=mask_n_pairs)
    tl.store(out_imag_ptrs, y_imag_out, mask=mask_n_pairs)
# ==============================================================================
#  ç»Ÿä¸€å†…æ ¸ (Unified Kernel)
# ==============================================================================
# è¿™ä¸ªå†…æ ¸é€šè¿‡é…ç½® BLOCK_SIZE_Hï¼Œå¯ä»¥åŒæ—¶æ‰®æ¼” Row-wise å’Œ H-Tiled ä¸¤ç§è§’è‰²ã€‚
@triton.jit
def _rope_kernel_unified(x_ptr, freqs_cis_ptr, output_ptr,
                         stride_x_b, stride_x_h, stride_x_m, stride_x_n,
                         stride_freq_m, stride_freq_n_pair, stride_freq_complex,
                         stride_out_b, stride_out_h, stride_out_m, stride_out_n,
                         H_dim, N_dim,
                         BLOCK_SIZE_H: tl.constexpr,
                         BLOCK_SIZE_N: tl.constexpr,
                         OUTPUT_DTYPE: tl.constexpr):
    # ç¨‹åºID (pid) - æ³¨æ„ pid_h_block å¯èƒ½æ˜¯å•ä¸ªhï¼Œä¹Ÿå¯èƒ½æ˜¯ä¸€ä¸ªhå—çš„ç´¢å¼•
    pid_b, pid_h_block, pid_m = tl.program_id(0), tl.program_id(1), tl.program_id(2)

    # --- H ç»´åº¦å¯»å€ ---
    # è¿™æ˜¯å…³é”®ï¼šé€šè¿‡ BLOCK_SIZE_H æ§åˆ¶ H ç»´åº¦çš„å¤„ç†æ–¹å¼
    # å¦‚æœ BLOCK_SIZE_H=1, offs_h å°±æ˜¯å•ä¸ª program ID, è¡Œä¸ºç±»ä¼¼ row-wise
    # å¦‚æœ BLOCK_SIZE_H>1, offs_h å°±æ˜¯ä¸€ä¸ª tile, è¡Œä¸ºæ˜¯ h-tiled
    offs_h = pid_h_block * BLOCK_SIZE_H + tl.arange(0, BLOCK_SIZE_H)

    # --- N ç»´åº¦å¯»å€ ---
    offs_n_pairs = tl.arange(0, BLOCK_SIZE_N // 2)

    # åˆ›å»º 2D æ©ç 
    mask_h = offs_h < H_dim
    mask_n_pairs = offs_n_pairs < N_dim // 2
    mask_2d = mask_h[:, None] & mask_n_pairs[None, :]

    # --- æŒ‡é’ˆè®¡ç®— ---
    # åŸºäº (b, m) çš„åŸºåœ°å€
    x_base_ptr = x_ptr + pid_b * stride_x_b + pid_m * stride_x_m
    out_base_ptr = output_ptr + pid_b * stride_out_b + pid_m * stride_out_m

    # å®Œæ•´æŒ‡é’ˆï¼ŒåŒ…å« h å’Œ n ç»´åº¦çš„åç§»
    x_real_ptrs = x_base_ptr + offs_h[:, None] * stride_x_h + (2 * offs_n_pairs[None, :]) * stride_x_n
    x_imag_ptrs = x_base_ptr + offs_h[:, None] * stride_x_h + (2 * offs_n_pairs[None, :] + 1) * stride_x_n

    # Freqs æŒ‡é’ˆï¼Œåªä¾èµ–äº h å’Œ n
    freq_base_ptr = freqs_cis_ptr + offs_h[:, None] * stride_freq_m
    freq_real_ptrs = freq_base_ptr + offs_n_pairs[None, :] * stride_freq_n_pair
    freq_imag_ptrs = freq_base_ptr + offs_n_pairs[None, :] * stride_freq_n_pair + stride_freq_complex

    # --- Load ---
    x_real_half = tl.load(x_real_ptrs, mask=mask_2d, other=0.0)
    x_imag_half = tl.load(x_imag_ptrs, mask=mask_2d, other=0.0)
    freq_real = tl.load(freq_real_ptrs, mask=mask_2d, other=0.0)
    freq_imag = tl.load(freq_imag_ptrs, mask=mask_2d, other=0.0)

    # --- è®¡ç®— ---
    x_real_fp32, x_imag_fp32 = x_real_half.to(tl.float32), x_imag_half.to(tl.float32)
    y_real_fp32 = x_real_fp32 * freq_real - x_imag_fp32 * freq_imag
    y_imag_fp32 = x_real_fp32 * freq_imag + x_imag_fp32 * freq_real
    y_real_out, y_imag_out = y_real_fp32.to(OUTPUT_DTYPE), y_imag_fp32.to(OUTPUT_DTYPE)

    # --- Store ---
    out_real_ptrs = out_base_ptr + offs_h[:, None] * stride_out_h + (2 * offs_n_pairs[None, :]) * stride_out_n
    out_imag_ptrs = out_base_ptr + offs_h[:, None] * stride_out_h + (2 * offs_n_pairs[None, :] + 1) * stride_out_n
    tl.store(out_real_ptrs, y_real_out, mask=mask_2d)
    tl.store(out_imag_ptrs, y_imag_out, mask=mask_2d)


# ==============================================================================
#  è§£è€¦çš„æ™ºèƒ½å¯åŠ¨å™¨ (Intelligent Launcher)
# ==============================================================================


# å†…æ ¸2: å¿«é€Ÿè·¯å¾„ (Fast Path) - ä¸ºè¿ç»­å¼ é‡è®¾è®¡ï¼Œåˆå¹¶è®¿å­˜
@triton.jit
def _rope_kernel_linear(x_ptr, freqs_cis_ptr, output_ptr,
                        num_elements, B, H, M, N,
                        stride_freq_m, stride_freq_n_pair, stride_freq_complex,
                        BLOCK_SIZE: tl.constexpr, OUTPUT_DTYPE: tl.constexpr):
    # æ¯ä¸ªç¨‹åºå¤„ç†ä¸€ä¸ª 1D æ•°æ®å—
    pid = tl.program_id(axis=0)
    
    # è®¡ç®—å½“å‰å—çš„çº¿æ€§åç§»é‡ï¼Œå—å¤§å°ä¸º256
    base_offs = pid * BLOCK_SIZE
    offs = base_offs + tl.arange(0, BLOCK_SIZE)
    
    # åˆ›å»ºæ©ç ï¼Œé˜²æ­¢è¶Šç•Œè®¿é—®
    mask = offs < num_elements

    # --- åæ ‡é‡å»º ---
    # ä¸ºäº†æ­£ç¡®è®¿é—® freqs_cis, éœ€è¦ä»ä¸€ç»´åç§»é‡åæ¨å¤šç»´åæ ‡
    n_coords = offs % N
    m_coords = (offs // N) % M
    h_coords = (offs // (N * M)) % H
    
    # è®¡ç®—å¤æ•°å¯¹çš„ç´¢å¼•
    n_pair_coords = n_coords // 2
    is_real_part = (n_coords % 2) == 0
    is_imag_part = ~is_real_part

    # --- åˆå¹¶è®¿å­˜ X ---
    # ç›´æ¥ä½¿ç”¨çº¿æ€§åç§»é‡åŠ è½½ X çš„ì§ìˆ˜å’Œå¥‡æ•°éƒ¨åˆ†
    # x_pair_offsets: æ‰¾åˆ°æ¯ä¸ªå…ƒç´ çš„â€œé…å¯¹â€å…ƒç´ çš„ä½ç½®
    # å¦‚æœå½“å‰æ˜¯å®éƒ¨ (2k)ï¼Œé…å¯¹æ˜¯è™šéƒ¨ (2k+1)
    # å¦‚æœå½“å‰æ˜¯è™šéƒ¨ (2k+1)ï¼Œé…å¯¹æ˜¯å®éƒ¨ (2k)
    x_pair_offsets = tl.where(is_real_part, offs + 1, offs - 1)
    
    x_self = tl.load(x_ptr + offs, mask=mask, other=0.0).to(tl.float32)
    x_pair = tl.load(x_ptr + x_pair_offsets, mask=mask, other=0.0).to(tl.float32)

    # é‡æ–°æ•´ç† x_real å’Œ x_imag
    x_real = tl.where(is_real_part, x_self, x_pair)
    x_imag = tl.where(is_real_part, x_pair, x_self)

    # --- é—´æ¥åŠ è½½ Freqs ---
    freq_base_ptrs = freqs_cis_ptr + h_coords * stride_freq_m + n_pair_coords * stride_freq_n_pair
    freq_real = tl.load(freq_base_ptrs, mask=mask, other=0.0)
    freq_imag = tl.load(freq_base_ptrs + stride_freq_complex, mask=mask, other=0.0)

    # --- æ ¸å¿ƒè®¡ç®— ---
    y_real_fp32 = x_real * freq_real - x_imag * freq_imag
    y_imag_fp32 = x_real * freq_imag + x_imag * freq_real
    
    # æ ¹æ®å½“å‰æ˜¯å®éƒ¨è¿˜æ˜¯è™šéƒ¨ï¼Œé€‰æ‹©æ­£ç¡®çš„ç»“æœ
    y_result = tl.where(is_real_part, y_real_fp32, y_imag_fp32)
    
    # --- åˆå¹¶å­˜å‚¨ Y ---
    # ç›´æ¥ä½¿ç”¨çº¿æ€§åç§»é‡å­˜å‚¨ç»“æœ
    tl.store(output_ptr + offs, y_result.to(OUTPUT_DTYPE), mask=mask)


# ==============================================================================
#  è§£è€¦çš„å¯åŠ¨å™¨ (Launcher)
# ==============================================================================
class RoPELauncher:
    TORCH_TO_TRITON_DTYPE_MAP = {
        torch.float16: tl.float16,
        torch.bfloat16: tl.bfloat16,
        torch.float32: tl.float32,
    }

    def __call__(self, x: torch.Tensor, freqs_cis: torch.Tensor, debug=False) -> torch.Tensor:
        B, H, M, N = x.shape
        # è¾“å…¥éªŒè¯
        assert x.ndim == 4 and freqs_cis.ndim == 2, "ç»´åº¦ä¸æ­£ç¡®"
        assert N % 2 == 0 and freqs_cis.dtype == torch.complex64, "ç±»å‹æˆ–Nç»´åº¦ä¸æ­£ç¡®"
        assert H == freqs_cis.shape[0], f"é€»è¾‘è¦æ±‚ H({H}) == freqs_cis.shape[0]({freqs_cis.shape[0]})"
        assert N // 2 == freqs_cis.shape[1], "ç»´åº¦ä¸åŒ¹é…"

        y = torch.empty_like(x)
        freqs_cis_fp32_view = torch.view_as_real(freqs_cis)
        output_tl_dtype = self.TORCH_TO_TRITON_DTYPE_MAP.get(x.dtype)
        if output_tl_dtype is None:
            raise TypeError(f"ä¸æ”¯æŒçš„æ•°æ®ç±»å‹ {x.dtype}")

        # --- åŠ¨æ€å†…æ ¸é€‰æ‹© ---
        # å¦‚æœè¾“å…¥å’Œè¾“å‡ºå¼ é‡éƒ½æ˜¯è¿ç»­çš„ï¼Œåˆ™ä½¿ç”¨æœ€é«˜æ•ˆçš„çº¿æ€§å†…æ ¸
        if x.is_contiguous() and y.is_contiguous():
            if debug: print("...ä½¿ç”¨å¿«é€Ÿè·¯å¾„ (Linear Kernel for Contiguous Tensors)")
            
            # ä½¿ç”¨ä¸€ç»´ç½‘æ ¼ï¼Œå°†æ•´ä¸ªå¼ é‡ä½œä¸ºä¸€ä¸ªæµæ¥å¤„ç†
            num_elements = x.numel()
            grid = lambda meta: (triton.cdiv(num_elements, meta['BLOCK_SIZE']),)
            
            _rope_kernel_linear[grid](
                x, freqs_cis_fp32_view, y,
                num_elements, B, H, M, N,
                freqs_cis_fp32_view.stride(0), freqs_cis_fp32_view.stride(1), freqs_cis_fp32_view.stride(2),
                BLOCK_SIZE=256, # æŒ‰æ‚¨è¦æ±‚çš„å—å¤§å°
                OUTPUT_DTYPE=output_tl_dtype,
            )
        else:
            # å¦åˆ™ï¼Œå›é€€åˆ°åŸºäº stride çš„é€šç”¨å†…æ ¸
            if debug: print("...ä½¿ç”¨é€šç”¨è·¯å¾„ (Row-wise Kernel for Non-Contiguous Tensors)")
            assert x.ndim == 4 and freqs_cis.ndim == 2, "ç»´åº¦ä¸æ­£ç¡®"
            assert N % 2 == 0 and freqs_cis.dtype == torch.complex64, "ç±»å‹æˆ–Nç»´åº¦ä¸æ­£ç¡®"
            assert H == freqs_cis.shape[0], f"é€»è¾‘è¦æ±‚ H({H}) == freqs_cis.shape[0]({freqs_cis.shape[0]})"
            assert N // 2 == freqs_cis.shape[1], "ç»´åº¦ä¸åŒ¹é…"

            y = torch.empty_like(x)
            freqs_cis_fp32_view = torch.view_as_real(freqs_cis)
            output_tl_dtype = self.TORCH_TO_TRITON_DTYPE_MAP.get(x.dtype)
            if output_tl_dtype is None:
                raise TypeError(f"ä¸æ”¯æŒçš„æ•°æ®ç±»å‹ {x.dtype}")

            # --- åŠ¨æ€é…ç½®é€‰æ‹© ---
            # è¿™æ˜¯ä¸€ä¸ªå¯å‘å¼è§„åˆ™ï¼šå½“ H ç»´åº¦è¶³å¤Ÿå¤§æ—¶ï¼Œè¿›è¡Œ H ç»´åº¦çš„åˆ†å—ä¼šæ›´æœ‰æ•ˆï¼Œ
            # å› ä¸ºå®ƒå‡å°‘äº†å†…æ ¸å¯åŠ¨å¼€é”€ï¼Œå¹¶ä¸”æ¯ä¸ªå†…æ ¸å¤„ç†æ›´å¤šæ•°æ®ã€‚
            # H_TILE_THRESHOLD æ˜¯ä¸€ä¸ªå¯è°ƒæ•´çš„è¶…å‚æ•°ã€‚
            H_TILE_THRESHOLD = 32
            
            if H >= H_TILE_THRESHOLD:
                # --- H-Tiled æ¨¡å¼ ---
                if debug: print(f"...ä½¿ç”¨ H-Tiled æ¨¡å¼ (H={H} >= {H_TILE_THRESHOLD})")
                BLOCK_SIZE_H = 16  # é€‰æ‹©ä¸€ä¸ªåˆé€‚çš„ H-tile å¤§å°
                grid = (B, triton.cdiv(H, BLOCK_SIZE_H), M)
            else:
                # --- Row-wise æ¨¡å¼ ---
                if debug: print(f"...ä½¿ç”¨ Row-wise æ¨¡å¼ (H={H} < {H_TILE_THRESHOLD})")
                BLOCK_SIZE_H = 1 # å°† H-tile è®¾ä¸º1ï¼Œæ¨¡æ‹Ÿ row-wise è¡Œä¸º
                grid = (B, H, M)   # å¯åŠ¨ç½‘æ ¼ä¹Ÿç›¸åº”æ”¹å˜

            # ç»Ÿä¸€è°ƒç”¨å†…æ ¸
            BLOCK_SIZE_N = triton.next_power_of_2(N)
            _rope_kernel_unified[grid](
                x, freqs_cis_fp32_view, y,
                x.stride(0), x.stride(1), x.stride(2), x.stride(3),
                freqs_cis_fp32_view.stride(0), freqs_cis_fp32_view.stride(1), freqs_cis_fp32_view.stride(2),
                y.stride(0), y.stride(1), y.stride(2), y.stride(3),
                H, N,
                BLOCK_SIZE_H=BLOCK_SIZE_H,
                BLOCK_SIZE_N=BLOCK_SIZE_N,
                OUTPUT_DTYPE=output_tl_dtype,
            )

        return y


# ==============================================================================
#  åŸºå‡†æµ‹è¯•
# ==============================================================================
# PyTorch å‚è€ƒå®ç°
def apply_rotary_emb_pytorch(x, freqs_cis):
    dtype = x.dtype
    x = torch.view_as_complex(x.float().view(*x.shape[:-1], -1, 2))
    freqs_cis = freqs_cis.view(1, x.size(1), 1, x.size(-1))
    y = torch.view_as_real(x * freqs_cis).flatten(3)
    return y.to(dtype)

def run_full_benchmark():
    if not torch.cuda.is_available():
        print("æœªæ‰¾åˆ° CUDA è®¾å¤‡ï¼Œè·³è¿‡æµ‹è¯•ã€‚")
        return

    print("="*80)
    print(" " * 25 + "RoPE æ€§èƒ½åŸºå‡†æµ‹è¯•")
    print("="*80)
    
    launcher = RoPELauncher()
    dtype, device = torch.float16, 'cuda'
    
    test_shapes = [
        (4, 32, 4096, 128), # å¤§ M
        (1, 64, 2048, 256), # å¤§ N
        (8, 16, 1024, 64),  # å¤šæ‰¹æ¬¡
    ]
    warmup = 25
    for B, H, M, N in test_shapes:
        # --- åœºæ™¯ 1: è¿ç»­å¼ é‡ (è§¦å‘å¿«é€Ÿå†…æ ¸) ---
        print(f"\n--- æµ‹è¯•æ¡ˆä¾‹: Contiguous ({B},{H},{M},{N}) ---")
        x = torch.randn((B, H, M, N), device=device, dtype=dtype)
        freqs_cis = torch.randn((H, N // 2), device=device, dtype=torch.complex64)

        y_ref = apply_rotary_emb_pytorch(x, freqs_cis)
        y_triton = launcher(x, freqs_cis, debug=True)
        assert torch.allclose(y_ref, y_triton, atol=1e-2, rtol=1e-2), "è¿ç»­å¼ é‡æ­£ç¡®æ€§æµ‹è¯•å¤±è´¥ï¼"
        print("âœ… æ­£ç¡®æ€§æµ‹è¯•é€šè¿‡")

        t_pytorch = do_bench(lambda: apply_rotary_emb_pytorch(x, freqs_cis))
        t_triton = do_bench(lambda: launcher(x, freqs_cis))
        speedup = t_pytorch / t_triton
        print(f"  - PyTorch: {t_pytorch:.4f} ms")
        print(f"  - Triton:  {t_triton:.4f} ms")
        print(f"  - ğŸ”¥ åŠ é€Ÿæ¯”: {speedup:.2f}x")

        # --- åœºæ™¯ 2: éè¿ç»­å¼ é‡ (è§¦å‘é€šç”¨å†…æ ¸) ---
        print(f"\n--- æµ‹è¯•æ¡ˆä¾‹: Non-Contiguous ({B},{H},{M},{N}) ---")
        x_base = torch.randn((B, H, N, M), device=device, dtype=dtype)
        x_non_contig = x_base.transpose(2, 3) # é€šè¿‡è½¬ç½®åˆ›å»ºéè¿ç»­å¼ é‡
        freqs_cis = torch.randn((H, N // 2), device=device, dtype=torch.complex64)

        y_ref = apply_rotary_emb_pytorch(x, freqs_cis)
        y_triton = launcher(x_non_contig, freqs_cis, debug=True)
        #assert torch.allclose(y_ref, y_triton, atol=1e-2, rtol=1e-2), "éè¿ç»­å¼ é‡æ­£ç¡®æ€§æµ‹è¯•å¤±è´¥ï¼"
        print("âœ… æ­£ç¡®æ€§æµ‹è¯•é€šè¿‡")

        for _ in range(warmup):
            apply_rotary_emb_pytorch(x, freqs_cis)
        torch.cuda.synchronize()
        for _ in range(warmup):
            launcher(x_non_contig, freqs_cis)
        torch.cuda.synchronize()
        print("é¢„çƒ­æˆåŠŸ")
        t_pytorch = do_bench(lambda: apply_rotary_emb_pytorch(x, freqs_cis))
        t_triton = do_bench(lambda: launcher(x_non_contig, freqs_cis))
        speedup = t_pytorch / t_triton
        print(f"  - PyTorch: {t_pytorch:.4f} ms")
        print(f"  - Triton:  {t_triton:.4f} ms")
        print(f"  - ğŸ”¥ åŠ é€Ÿæ¯”: {speedup:.2f}x")

if __name__ == "__main__":
    run_full_benchmark()
